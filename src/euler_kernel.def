/*  Copyright 2011 Alexis Herault, Giuseppe Bilotta, Robert A. Dalrymple, Eugenio Rustico, Ciro Del Negro

	Istituto de Nazionale di Geofisica e Vulcanologia
          Sezione di Catania, Catania, Italy

    Universita di Catania, Catania, Italy

    Johns Hopkins University, Baltimore, MD

    This file is part of GPUSPH.

    GPUSPH is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    GPUSPH is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with GPUSPH.  If not, see <http://www.gnu.org/licenses/>.
*/

/* Euler kernel definitions */
// Predictor Corrector time integration
// - for step 1:
//	  v(n+1/2) = v(n) + f(n)*dt/2
//	  pos(n+1/2) = pos(n) + v(n)*dt/2
//
//	  We have oldVel = v(n), oldPos = pos(n), forces = f(n) so
//	  newVel = v(n+1/2) = oldVel + forces*dt/2
//	  newPos = pos(n+1/2) = oldPos + oldVel*dt/2
//
// - for step 2:
//	  vc(n+1/2) = v(n) + f(n+1/2)*dt/2
//	  posc(n+1/2) = pos(n) + vc(n+1/2)*dt/2
//	  then:
//	  v(n+1) = 2vc(n+1/2) - v(n) = v(n) + f(n+1/2)*dt
//	  pos(n+1) = 2posc(n+1/2) - pos(n) = pos(n) + vc(n+1/2)*dt
//
//	  Whe have oldVel = v(n), oldPos = pos(n), force = f(n+1/2),
//	  newVel = vel(n+1/2), newPos = pos(n+1/2) so
//	  we store velc = v(n) + f(n+1/2)*dt/2 then
//	  newPos = pos(n+1) = oldPos + velc*dt
//	  newVel = vel(n+1) = oldVel + forces*dt;

// Remember that for step 1 dt => dt/2 and for step 2 dt => dt !!!
// but dt2 is always equal to dt/2

template<int step, int periodicbound>
__global__ void
EULER_KERNEL_NAME(
	float4*	oldPos,
	float4*	oldVel,
	particleinfo* info,
	float4*	forces,
#if XSPH_KERNEL
	float4*	xsph,
#endif
	float4*	newPos,
	float4*	newVel,
	uint	numParticles,
	float	dt,
	float	dt2,
	float	t)
{
	int index = INTMUL(blockIdx.x,blockDim.x) + threadIdx.x;

	if (index >= numParticles)
		return;

	// read particle data from sorted arrays
	// Euler does nothing to boundary particles apart
	// copying pos and vel in the new arrays
	float4 pos = oldPos[index];		// always pos(n)
	float4 vel = oldVel[index];		// always vel(n)

	/* Since we never use the last two components of pinfo, the compiler is
	   ‘smart’ enough to not load them at all. This kills coalescence on
	   older hardware, because each thread reads 32 bits, with a 32-bit gap
	   between one thread and the next.
	   We can work around this by forcing the compiler to code a read for
	   a 64-bit word by reading a long int at our interested address,
	   and then casting the result to particleinfo.
	   This speeds up the euler kernel by 20% on 1.1 devices, and by
	   only doing it for older hardware we spare ourselves the very small
	   performance loss that we'd find on 2.0 devices with the hack. */
	#if __GPU_ARCH__ < 200
	long int hack = *reinterpret_cast<long int*>(info + index);
	particleinfo pinfo = *reinterpret_cast<particleinfo*>(&hack);
	#else
	particleinfo pinfo = info[index];
	#endif

	if(type(pinfo) != BOUNDPART && type(pinfo) != VERTEXPART) {
		float4 force = forces[index];	// f(n) at step 1 and f(n+1/2) at step 2
		# ifdef XSPH_KERNEL
		float4 mean_vel = xsph[index];
		# endif
		/*
		   velc = vel if step == 1, but
		   velc = vel + forces[index]*dt/2.0f if step == 2
		 */
		float4 velc = vel + (step - 1)*forces[index]*dt2;

		// Updating particle position
		if (FLUID(pinfo)) {
			# ifdef XSPH_KERNEL
			pos.x +=  (velc.x + d_epsxsph*mean_vel.x)*dt;
			pos.y +=  (velc.y + d_epsxsph*mean_vel.y)*dt;
			pos.z +=  (velc.z + d_epsxsph*mean_vel.z)*dt;
			# else
			pos.x +=  velc.x*dt;
			pos.y +=  velc.y*dt;
			pos.z +=  velc.z*dt;
			# endif

			// Updating particle velocity
			// For step 1:
			//	  vel = vel(n+1/2) = vel(n) + f(n)*dt/2
			// For step 2:
			//	  vel = vel(n+1) = vel(n) + f(n+1/2)*dt
			vel += dt*force;
		}
		// Updating postions for Piston particles
		else if (type(pinfo) == PISTONPART) {
			const int i = object(pinfo);
			pos.x = d_mbdata[i].x;
		}
		// Updating postions for Paddle particles
		else if (type(pinfo)  == PADDLEPART) {
			const int i = object(pinfo);
			const float r = length(make_float2(pos.x - d_mbdata[i].x, pos.z - d_mbdata[i].y));
			pos.x = d_mbdata[i].x + r*d_mbdata[i].z;	// r*sin(theta(t))
			pos.z = d_mbdata[i].y + r*d_mbdata[i].w;	// r*cos(theta(t))
		}
		// Updating postions for Gate particles
		// At each substep the gate part covers a distance equals to v*dt/2.0
		// so in 2 substep the gate covers v*dt BUT pos is always pos(n) so
		// at step 1 we have pos += v*dt with dt=dt/2 and at step 2 we have
		// pos += v*dt with dt = dt
		else if (type(pinfo)  == GATEPART) {
			const int i = object(pinfo);
			pos.x += d_mbdata[i].x*dt;
			pos.y += d_mbdata[i].y*dt;
			pos.z += d_mbdata[i].z*dt;
		}
		// TODO: test rotations
		else if (type(pinfo) == OBJECTPART) {
			const int i = object(pinfo);

			// Applying rotation around center of gravity
			applyrot(&d_rbsteprot[9*i], pos, d_rbcg[i]);

			// Applying center of gravity translation
			pos.x += d_rbtrans[i].x;
			pos.y += d_rbtrans[i].y;
			pos.z += d_rbtrans[i].z;
		}

		if (periodicbound) {
			if (d_dispvect.x) {
				if (pos.x >= d_maxlimit.x)
					pos.x -= d_dispvect.x;
				else if (pos.x < d_minlimit.x)
					pos.x += d_dispvect.x;
			}
			if (d_dispvect.y) {
				if (pos.y >= d_maxlimit.y)
					pos.y -= d_dispvect.y;
				else if (pos.y < d_minlimit.y)
					pos.y += d_dispvect.y;
			}
			if (d_dispvect.z) {
				if (pos.z >= d_maxlimit.z)
					pos.z -= d_dispvect.z;
				else if (pos.z < d_minlimit.z)
					pos.z += d_dispvect.z;
			}
		}
	}
	newPos[index] = pos;
	newVel[index] = vel;
}

/* vi:set ft=cuda: */
